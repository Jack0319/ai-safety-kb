Metadata-Version: 2.4
Name: ai-safety-knowledge-base
Version: 0.1.0
Summary: Structured AI safety knowledge base with ingestion and retrieval APIs.
Author: AI Safety Collective
License: MIT
Project-URL: homepage, https://github.com/example/ai-safety-knowledge-base
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pydantic>=2.6.0
Requires-Dist: pydantic-settings>=2.2.1
Requires-Dist: sqlalchemy>=2.0.30
Requires-Dist: asyncpg>=0.29.0
Requires-Dist: aiosqlite>=0.20.0
Requires-Dist: pgvector>=0.2.5
Requires-Dist: httpx>=0.27.0
Requires-Dist: python-dotenv>=1.0.1
Requires-Dist: pytest-asyncio>=0.23.0
Requires-Dist: PyPDF2>=3.0.0
Provides-Extra: ingestion
Requires-Dist: beautifulsoup4>=4.12.3; extra == "ingestion"
Requires-Dist: feedparser>=6.0.11; extra == "ingestion"
Requires-Dist: sentence-transformers>=3.0.0; extra == "ingestion"
Dynamic: license-file

# AI Safety Knowledge Base

An open-source, curated, structured, and searchable AI safety corpus powering safety-aware agents, MCP tools, and alignment research workflows.

## What This Repository Provides

- **Ingestion pipelines** that normalize Alignment Forum posts, arXiv papers, AI Incident Database entries, governance reports, and internal docs into a unified schema with provenance, versioning, and source records.
- **Storage layer** built on SQL databases (SQLite for dev, Postgres+pgvector for production) with chunk-level embeddings and metadata filters.
- **Source registry + catalog** so every document traces back to a per-source record with ingestion mode, status, doc counts, and canonical URLs. Auto-generate `sources_catalog.md` to keep a living “links” page in sync with the registry.
- **Retrieval API** (`safety_kb.retrieval`) exposing async helpers such as `search`, `get_document`, `get_chunks_for_document`, `search_by_topic`, and `list_topics`. These functions return Pydantic models that MCP tools can wrap directly.
- **Utilities** for chunking, text cleaning, embedding management, logging, and ingestion orchestration.
- **Tests and migrations** to keep the KB stable as it scales to tens of thousands of documents and millions of chunks.

## Project Layout

```
ai-safety-knowledge-base/
├── env.example                # Copy to .env to override defaults
├── pyproject.toml             # PEP 621 metadata + dependencies
├── README.md
├── migrations/
│   └── 001_init.sql           # Reference SQL definition for Postgres/pgvector
├── src/
│   └── safety_kb/
│       ├── __init__.py
│       ├── config.py          # Settings + helpers
│       ├── models.py          # Pydantic domain models
│       ├── schemas.py         # SQLAlchemy ORM tables
│       ├── storage.py         # Async persistence helpers
│       ├── retrieval.py       # Public KB API
│       ├── indexing.py        # Ingestion + indexing orchestration
│       ├── sources/           # Source-specific fetchers
│       ├── catalog.py         # Source catalog generator (produces sources_catalog.md)
│       └── utils/             # Text cleaning, chunking, embeddings, logging, checksums
└── tests/
    ├── test_indexing.py
    ├── test_ingestion_sources.py
    └── test_retrieval.py
```

## Getting Started

1. **Install dependencies**

   ```bash
   pip install -e .
   ```

2. **Configure settings**

   Copy `env.example` to `.env` (or set environment variables) and update values such as `SAFETY_KB_DATABASE_URL` and embedding provider config.

3. **Create the database**

   For local dev:

   ```bash
   python -m safety_kb.storage --init-db
   ```

   For production Postgres, apply `migrations/001_init.sql` and ensure `pgvector` is enabled.

4. **Run tests**

   ```bash
   pytest
   ```

5. **Add new sources via catalog or local files**

   - **Web links**: edit `sources_catalog.md` and add a table row with the source name, kind (e.g., `website`), mode, and a Markdown hyperlink in the last column.
   - **Local documents**: drop `.txt`, `.md`, `.html`, or `.pdf` files into `sources/files/`.
   - Then run:

     ```bash
     python -m safety_kb.catalog_sync --catalog sources_catalog.md --sources-dir sources/files
     ```

     This command:

     1. Parses the catalog rows, upserts those sources, and (for `website` kinds) fetches/ingests their content.
     2. Discovers new files under `sources/files/`, ingests them, and adds rows to the catalog with working hyperlinks.
     3. Re-generates `sources_catalog.md` from the live registry so the page always reflects what is actually ingested.

   Commit both the ingested files (if desired) and the refreshed catalog.

## Integration With MCP

The `safety_kb.retrieval` module is purposely thin so that MCP tool definitions stay simple:

```python
from safety_kb.retrieval import search, get_document

@server.tool(name="kb.search")
async def kb_search(request):
    results = await search(
        query=request.input["query"],
        k=request.input.get("k", 5),
        filters=request.input.get("filters", {}),
    )
    return {"results": [r.model_dump() for r in results]}
```

## Roadmap

- Add production-grade ingestion connectors (Alignment Forum, arXiv, AIID, etc.).
- Expand observability (metrics, structured logs, tracing).
- Support cached embeddings and queued ingestion with workers (Celery/Arq).
- Introduce more advanced reranking (cross-encoders) and hybrid BM25 + ANN pipelines.
